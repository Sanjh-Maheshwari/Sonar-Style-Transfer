{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "2WzJRnlZIoX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd85516-06cf-4c14-e458-13b1130ca383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec 18 12:19:32 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P0    29W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-fid"
      ],
      "metadata": {
        "id": "fbzUdA2xczWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec657ca-8468-4e36-e531-4d02432cf471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-fid\n",
            "  Downloading pytorch-fid-0.2.1.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pytorch-fid) (1.21.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from pytorch-fid) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from pytorch-fid) (1.7.3)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-fid) (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-fid) (0.14.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.1->pytorch-fid) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.2.2->pytorch-fid) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.2.2->pytorch-fid) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.2.2->pytorch-fid) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.2.2->pytorch-fid) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.2.2->pytorch-fid) (1.24.3)\n",
            "Building wheels for collected packages: pytorch-fid\n",
            "  Building wheel for pytorch-fid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-fid: filename=pytorch_fid-0.2.1-py3-none-any.whl size=14834 sha256=f28f7e38d68264efda682e57433a7063710f5b4cf21a2dfd5c8a47ac6a07805f\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/c8/a0/cce2ed7671ae52be132ae836e429bba6148544f83b7962b4bc\n",
            "Successfully built pytorch-fid\n",
            "Installing collected packages: pytorch-fid\n",
            "Successfully installed pytorch-fid-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BkNS5ScsbMHh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93702c45-9b01-4e5d-9f5f-b880a0feabca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Style transfer'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0akAJO5fbM-",
        "outputId": "bbe73c89-9ad8-484e-b648-f710334163cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Style transfer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lAir7pXSfibs",
        "outputId": "786d8b73-413f-465c-c1c6-096bb6e15b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Style transfer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zrq2BfaDam4g"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/gordicaleksa/pytorch-neural-style-transfer.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd 'pytorch-neural-style-transfer'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHdV5_zDasmd",
        "outputId": "918f2b60-3d6c-4999-bf6a-0e801b306bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Style transfer/pytorch-neural-style-transfer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND8F6W4ea1DY",
        "outputId": "5c40a082-3b2d-4370-ad31-49541fdde9d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/            neural_style_transfer.py\n",
            "environment.yml  README.md\n",
            "LICENCE          reconstruct_image_from_representation.py\n",
            "\u001b[01;34mmodels\u001b[0m/          \u001b[01;34mutils\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import utils.utils as utils\n",
        "from utils.video_utils import create_video_from_intermediate_results\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam, LBFGS\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "default_resource_dir = '/content/drive/MyDrive/Style transfer/pytorch-neural-style-transfer/data'\n",
        "content_images_dir = os.path.join(default_resource_dir, 'content-images')\n",
        "style_images_dir = os.path.join(default_resource_dir, 'style-images')\n",
        "output_img_dir = os.path.join(default_resource_dir, 'output-images')\n",
        "img_format = (4, '.jpg')  # saves images in the format: %04d.jpg\n",
        "\n",
        "optimization_config = dict()\n",
        "optimization_config['height'] = 400\n",
        "optimization_config['content_weight'] = 1e7\n",
        "optimization_config['style_weight'] = 3e4 # 2 * 10^4 init <- 4 x 1o^4 \n",
        "optimization_config['tv_weight'] = 1e0\n",
        "optimization_config['optimizer'] = 'lbfgs'\n",
        "optimization_config['model'] = 'vgg19'\n",
        "optimization_config['init_method'] = 'content'\n",
        "optimization_config['saving_freq'] = -1\n",
        "\n",
        "# init config : (cw : 1e7, sw : 4e4, tw : 1e0)\n",
        "# new config : (cw : 1e7, sw : 5e4, tw : 1e-1)\n",
        "\n",
        "optimization_config['content_images_dir'] = content_images_dir\n",
        "optimization_config['style_images_dir'] = style_images_dir\n",
        "optimization_config['output_img_dir'] = output_img_dir\n",
        "optimization_config['img_format'] = img_format\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "neural_net, content_feature_maps_index_name, style_feature_maps_indices_names = utils.prepare_model('vgg19', device)\n",
        "\n",
        "def build_loss(neural_net, optimizing_img, target_representations, content_feature_maps_index, style_feature_maps_indices, config):\n",
        "    target_content_representation = target_representations[0]\n",
        "    target_style_representation = target_representations[1]\n",
        "\n",
        "    current_set_of_feature_maps = neural_net(optimizing_img)\n",
        "\n",
        "    current_content_representation = current_set_of_feature_maps[content_feature_maps_index].squeeze(axis=0)\n",
        "    content_loss = torch.nn.MSELoss(reduction='mean')(target_content_representation, current_content_representation)\n",
        "\n",
        "    style_loss = 0.0\n",
        "    current_style_representation = [utils.gram_matrix(x) for cnt, x in enumerate(current_set_of_feature_maps) if cnt in style_feature_maps_indices]\n",
        "    for gram_gt, gram_hat in zip(target_style_representation, current_style_representation):\n",
        "        style_loss += torch.nn.MSELoss(reduction='sum')(gram_gt[0], gram_hat[0])\n",
        "    style_loss /= len(target_style_representation)\n",
        "\n",
        "    tv_loss = utils.total_variation(optimizing_img)\n",
        "\n",
        "    total_loss = config['content_weight'] * content_loss + config['style_weight'] * style_loss + config['tv_weight'] * tv_loss\n",
        "\n",
        "    return total_loss, content_loss, style_loss, tv_loss\n",
        "\n",
        "\n",
        "def make_tuning_step(neural_net, optimizer, target_representations, content_feature_maps_index, style_feature_maps_indices, config):\n",
        "    # Builds function that performs a step in the tuning loop\n",
        "    def tuning_step(optimizing_img):\n",
        "        total_loss, content_loss, style_loss, tv_loss = build_loss(neural_net, optimizing_img, target_representations, content_feature_maps_index, style_feature_maps_indices, config)\n",
        "        # Computes gradients\n",
        "        total_loss.backward()\n",
        "        # Updates parameters and zeroes gradients\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        return total_loss, content_loss, style_loss, tv_loss\n",
        "\n",
        "    # Returns the function that will be called inside the tuning loop\n",
        "    return tuning_step\n",
        "\n",
        "\n",
        "def neural_style_transfer(config, content_img_path, style_img_path):\n",
        "    # content_img_path = os.path.join(config['content_images_dir'], config['content_img_name'])\n",
        "    # style_img_path = os.path.join(config['style_images_dir'], config['style_img_name'])\n",
        "\n",
        "    # out_dir_name = '/content/drive/MyDrive/Style transfer/pytorch-neural-style-transfer/data/Generated-images'\n",
        "    dump_path = config['output_img_dir']\n",
        "    os.makedirs(dump_path, exist_ok=True)\n",
        "\n",
        "    content_img = utils.prepare_img(content_img_path, config['height'], device)\n",
        "    style_img = utils.prepare_img(style_img_path, config['height'], device)\n",
        "\n",
        "    if config['init_method'] == 'random':\n",
        "        # white_noise_img = np.random.uniform(-90., 90., content_img.shape).astype(np.float32)\n",
        "        gaussian_noise_img = np.random.normal(loc=0, scale=90., size=content_img.shape).astype(np.float32)\n",
        "        init_img = torch.from_numpy(gaussian_noise_img).float().to(device)\n",
        "    elif config['init_method'] == 'content':\n",
        "        init_img = content_img\n",
        "    else:\n",
        "        # init image has same dimension as content image - this is a hard constraint\n",
        "        # feature maps need to be of same size for content image and init image\n",
        "        style_img_resized = utils.prepare_img(style_img_path, np.asarray(content_img.shape[2:]), device)\n",
        "        init_img = style_img_resized\n",
        "\n",
        "    # we are tuning optimizing_img's pixels! (that's why requires_grad=True)\n",
        "    optimizing_img = Variable(init_img, requires_grad=True)\n",
        "\n",
        "    content_img_set_of_feature_maps = neural_net(content_img)\n",
        "    style_img_set_of_feature_maps = neural_net(style_img)\n",
        "\n",
        "    target_content_representation = content_img_set_of_feature_maps[content_feature_maps_index_name[0]].squeeze(axis=0)\n",
        "    target_style_representation = [utils.gram_matrix(x) for cnt, x in enumerate(style_img_set_of_feature_maps) if cnt in style_feature_maps_indices_names[0]]\n",
        "    target_representations = [target_content_representation, target_style_representation]\n",
        "\n",
        "    # magic numbers in general are a big no no - some things in this code are left like this by design to avoid clutter\n",
        "    num_of_iterations = {\n",
        "        \"lbfgs\": 1000,\n",
        "        \"adam\": 3000,\n",
        "    }\n",
        "\n",
        "    #\n",
        "    # Start of optimization procedure\n",
        "    #\n",
        "    if config['optimizer'] == 'adam':\n",
        "        optimizer = Adam((optimizing_img,), lr=1e1)\n",
        "        tuning_step = make_tuning_step(neural_net, optimizer, target_representations, content_feature_maps_index_name[0], style_feature_maps_indices_names[0], config)\n",
        "        for cnt in range(num_of_iterations[config['optimizer']]):\n",
        "            total_loss, content_loss, style_loss, tv_loss = tuning_step(optimizing_img)\n",
        "            with torch.no_grad():\n",
        "                print(f'Adam | iteration: {cnt:03}, total loss={total_loss.item():12.4f}, content_loss={config[\"content_weight\"] * content_loss.item():12.4f}, style loss={config[\"style_weight\"] * style_loss.item():12.4f}, tv loss={config[\"tv_weight\"] * tv_loss.item():12.4f}')\n",
        "                utils.save_and_maybe_display(optimizing_img, dump_path, config, cnt, num_of_iterations[config['optimizer']], should_display=False)\n",
        "    elif config['optimizer'] == 'lbfgs':\n",
        "        # line_search_fn does not seem to have significant impact on result\n",
        "        optimizer = LBFGS((optimizing_img,), max_iter=num_of_iterations['lbfgs'], line_search_fn='strong_wolfe')\n",
        "        cnt = 0\n",
        "\n",
        "        def closure():\n",
        "            nonlocal cnt\n",
        "            if torch.is_grad_enabled():\n",
        "                optimizer.zero_grad()\n",
        "            total_loss, content_loss, style_loss, tv_loss = build_loss(neural_net, optimizing_img, target_representations, content_feature_maps_index_name[0], style_feature_maps_indices_names[0], config)\n",
        "            if total_loss.requires_grad:\n",
        "                total_loss.backward()\n",
        "            with torch.no_grad():\n",
        "                if(cnt%200 == 0):\n",
        "                  print(f'L-BFGS | iteration: {cnt:03}, total loss={total_loss.item():12.4f}, content_loss={config[\"content_weight\"] * content_loss.item():12.4f}, style loss={config[\"style_weight\"] * style_loss.item():12.4f}, tv loss={config[\"tv_weight\"] * tv_loss.item():12.4f}')\n",
        "                utils.save_and_maybe_display(optimizing_img, dump_path, config, cnt, num_of_iterations[config['optimizer']], should_display=False, image_name = os.path.split(content_img_path)[1].split('.')[0] + '_' + os.path.split(style_img_path)[1].split('.')[0])\n",
        "\n",
        "            cnt += 1\n",
        "            return total_loss\n",
        "\n",
        "        optimizer.step(closure)\n",
        "\n",
        "    return dump_path\n",
        "\n",
        "    # uncomment this if you want to create a video from images dumped during the optimization procedure\n",
        "    # create_video_from_intermediate_results(results_path, img_format)"
      ],
      "metadata": {
        "id": "CZ9YR2zba2ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Final Style Transfer**"
      ],
      "metadata": {
        "id": "Clt7lu2Hvnlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2020)"
      ],
      "metadata": {
        "id": "-oslp4ssFDBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ship 1 and ship 2**"
      ],
      "metadata": {
        "id": "TbDb0j4Zvgoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# content_img_list = ['/content/drive/MyDrive/Style transfer/pytorch-neural-style-transfer/data/content-images/67.png']\n",
        "ship1_path = \"/content/drive/MyDrive/Style transfer/generated-images-models/ship1\"\n",
        "ship2_path = \"/content/drive/MyDrive/Style transfer/generated-images-models/ship2\"\n",
        "\n",
        "file_list_1 = []\n",
        "\n",
        "for root, dirs, files in os.walk(ship1_path):\n",
        "    for name in files:\n",
        "      file_list_1.append(os.path.join(root, name))\n",
        "\n",
        "for root, dirs, files in os.walk(ship2_path):\n",
        "    for name in files:\n",
        "      file_list_1.append(os.path.join(root, name))"
      ],
      "metadata": {
        "id": "9pXpndXxJl_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list_1[-10:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx3vR3OBwBiO",
        "outputId": "3d1f56f8-a105-4929-a277-fe913f3ac055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Style transfer/generated-images-models/ship2/17.png',\n",
              " '/content/drive/MyDrive/Style transfer/generated-images-models/ship2/6.png',\n",
              " '/content/drive/MyDrive/Style transfer/generated-images-models/ship2/3.png',\n",
              " '/content/drive/MyDrive/Style transfer/generated-images-models/ship2/2.png',\n",
              " '/content/drive/MyDrive/Style transfer/generated-images-models/ship2/4.png',\n",
              " '/content/drive/MyDrive/Style transfer/generated-images-models/ship2/5.png',\n",
              " '/content/drive/MyDrive/Style transfer/generated-images-models/ship2/0.png',\n",
              " '/content/drive/MyDrive/Style transfer/generated-images-models/ship2/1.png',\n",
              " '/content/drive/MyDrive/Style transfer/generated-images-models/ship2/7.png',\n",
              " '/content/drive/MyDrive/Style transfer/generated-images-models/ship2/8.png']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "content_img_list_first = np.random.choice(file_list_1, n, replace = False)"
      ],
      "metadata": {
        "id": "9BVmiSotMnX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_img_list_first[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIj5VxykM08-",
        "outputId": "427a9769-f368-41aa-e5e4-ed9c9137a333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['/content/drive/MyDrive/Style transfer/generated-images-models/ship1/24.png',\n",
              "       '/content/drive/MyDrive/Style transfer/generated-images-models/ship2/28.png',\n",
              "       '/content/drive/MyDrive/Style transfer/generated-images-models/ship2/74.png',\n",
              "       '/content/drive/MyDrive/Style transfer/generated-images-models/ship1/65.png',\n",
              "       '/content/drive/MyDrive/Style transfer/generated-images-models/ship1/122.png',\n",
              "       '/content/drive/MyDrive/Style transfer/generated-images-models/ship2/132.png',\n",
              "       '/content/drive/MyDrive/Style transfer/generated-images-models/ship2/2.png',\n",
              "       '/content/drive/MyDrive/Style transfer/generated-images-models/ship2/44.png',\n",
              "       '/content/drive/MyDrive/Style transfer/generated-images-models/ship2/133.png',\n",
              "       '/content/drive/MyDrive/Style transfer/generated-images-models/ship2/31.png'],\n",
              "      dtype='<U82')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "style_path = '/content/drive/MyDrive/Style transfer/pytorch-neural-style-transfer/data/style-images/ship-150.png'\n",
        "output_img_dir = os.path.join(default_resource_dir, 'ship1_2_new')\n",
        "optimization_config['style_img_name'] = os.path.basename(style_path)\n",
        "optimization_config['output_img_dir'] = output_img_dir\n",
        "\n",
        "t1 = time.time()\n",
        "\n",
        "for content_path in content_img_list_first:\n",
        "  print(f'Processing file : {os.path.split(content_path)[1]}')\n",
        "  optimization_config['content_img_name'] = os.path.basename(content_path)\n",
        "  results_path = neural_style_transfer(optimization_config, content_path, style_path)\n",
        "  print()\n",
        "\n",
        "t2 = time.time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "zGuaHWtsIkGZ",
        "outputId": "4e07eae2-24c1-4685-93e1-a3dc75adfac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file : 24.png\n",
            "L-BFGS | iteration: 000, total loss=1871196061696.0000, content_loss=      0.0000, style loss=1871193840000.0000, tv loss=2220095.0000\n",
            "L-BFGS | iteration: 200, total loss=94369497088.0000, content_loss=77596752929.6875, style loss=16745150625.0000, tv loss=27592228.0000\n",
            "L-BFGS | iteration: 400, total loss=88129937408.0000, content_loss=73913378906.2500, style loss=14185954687.5000, tv loss=30608566.0000\n",
            "L-BFGS | iteration: 600, total loss=86829654016.0000, content_loss=72953291015.6250, style loss=13842929062.5000, tv loss=33431948.0000\n",
            "L-BFGS | iteration: 800, total loss=86311272448.0000, content_loss=72568876953.1250, style loss=13706739375.0000, tv loss=35661724.0000\n",
            "L-BFGS | iteration: 1000, total loss=85996847104.0000, content_loss=72332338867.1875, style loss=13626879375.0000, tv loss=37636208.0000\n",
            "\n",
            "Processing file : 28.png\n",
            "L-BFGS | iteration: 000, total loss=2168155144192.0000, content_loss=      0.0000, style loss=2168153520000.0000, tv loss=1521166.0000\n",
            "L-BFGS | iteration: 200, total loss=131581419520.0000, content_loss=104544267578.1250, style loss=27009879375.0000, tv loss=27273072.0000\n",
            "L-BFGS | iteration: 400, total loss=120506466304.0000, content_loss=100506259765.6250, style loss=19971495000.0000, tv loss=28705184.0000\n",
            "L-BFGS | iteration: 600, total loss=117630509056.0000, content_loss=98964667968.7500, style loss=18634830000.0000, tv loss=31008396.0000\n",
            "L-BFGS | iteration: 800, total loss=116330012672.0000, content_loss=98113300781.2500, style loss=18183549375.0000, tv loss=33165672.0000\n",
            "L-BFGS | iteration: 1000, total loss=115518652416.0000, content_loss=97591220703.1250, style loss=17892316875.0000, tv loss=35108052.0000\n",
            "\n",
            "Processing file : 74.png\n",
            "L-BFGS | iteration: 000, total loss=2199589224448.0000, content_loss=      0.0000, style loss=2199587520000.0000, tv loss=1894589.0000\n",
            "\n",
            "Processing file : 65.png\n",
            "L-BFGS | iteration: 000, total loss=2203952611328.0000, content_loss=      0.0000, style loss=2203950240000.0000, tv loss=2369152.0000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-934c3fd3b21d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Processing file : {os.path.split(content_path)[1]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0moptimization_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content_img_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mresults_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_style_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimization_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-d70bf28d588e>\u001b[0m in \u001b[0;36mneural_style_transfer\u001b[0;34m(config, content_img_path, style_img_path)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdump_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_old\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_stps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0;31m# multiply by initial Hessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total time taken for processing {n} files : {t2 - t1}s')"
      ],
      "metadata": {
        "id": "k-64PPdAM_Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ship 3, 4 and 5**"
      ],
      "metadata": {
        "id": "Etwj4KkvxCdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# content_img_list = ['/content/drive/MyDrive/Style transfer/pytorch-neural-style-transfer/data/content-images/67.png']\n",
        "ship3_path = \"/content/drive/MyDrive/Style transfer/generated-images-models/ship3\"\n",
        "ship4_path = \"/content/drive/MyDrive/Style transfer/generated-images-models/ship4\"\n",
        "ship5_path = \"/content/drive/MyDrive/Style transfer/generated-images-models/ship5\"\n",
        "\n",
        "file_list_2 = []\n",
        "\n",
        "for root, dirs, files in os.walk(ship3_path):\n",
        "    for name in files:\n",
        "      file_list_2.append(os.path.join(root, name))\n",
        "\n",
        "for root, dirs, files in os.walk(ship4_path):\n",
        "    for name in files:\n",
        "      file_list_2.append(os.path.join(root, name))\n",
        "\n",
        "for root, dirs, files in os.walk(ship5_path):\n",
        "    for name in files:\n",
        "      file_list_2.append(os.path.join(root, name))"
      ],
      "metadata": {
        "id": "uirZYDLwxCdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list_2[-10:]"
      ],
      "metadata": {
        "id": "MDmm8hi7xCdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 50\n",
        "content_img_list_second = np.random.choice(file_list_2, n, replace = False)"
      ],
      "metadata": {
        "id": "jU8-WSxJxCdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_img_list_second[:10]"
      ],
      "metadata": {
        "id": "6ZhraAK6xCdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "style_path = '/content/drive/MyDrive/Style transfer/pytorch-neural-style-transfer/data/style-images/ship-030.png'\n",
        "output_img_dir = os.path.join(default_resource_dir, 'ship3_4_5_new')\n",
        "optimization_config['style_img_name'] = os.path.basename(style_path)\n",
        "optimization_config['output_img_dir'] = output_img_dir\n",
        "\n",
        "t1 = time.time()\n",
        "\n",
        "for content_path in content_img_list_second:\n",
        "  print(f'Processing file : {os.path.split(content_path)[1]}')\n",
        "  optimization_config['content_img_name'] = os.path.basename(content_path)\n",
        "  results_path = neural_style_transfer(optimization_config, content_path, style_path)\n",
        "  print()\n",
        "\n",
        "t2 = time.time()"
      ],
      "metadata": {
        "id": "COTA-Us1xCdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total time taken for processing {n} files : {t2 - t1}s')"
      ],
      "metadata": {
        "id": "DRvv1Bp6xCdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation**"
      ],
      "metadata": {
        "id": "4cmuGBdOI01Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsMv1sVcDv3b",
        "outputId": "69585ec4-b46d-4c69-e34c-361e93e3b7ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/            neural_style_transfer.py\n",
            "environment.yml  README.md\n",
            "LICENCE          reconstruct_image_from_representation.py\n",
            "\u001b[01;34mmodels\u001b[0m/          \u001b[01;34mutils\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Style transfer"
      ],
      "metadata": {
        "id": "dzT2ZJ0BY3_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92bfb8ad-afe8-4ed4-98e3-a57fe541ec56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1kQzhYFmZ6JrdPsG4tM0vxDYVfuEAcIKV/Style transfer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m pytorch_fid Generated-images  plane-real"
      ],
      "metadata": {
        "id": "-b-WS9IPDv3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! git clone https://github.com/mseitzer/pytorch-fid.git"
      ],
      "metadata": {
        "id": "6xSAytY6ZIui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8955df75-793e-4d8e-8a48-6895d52875ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pytorch-fid' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FID score between two different generated images using different stype images\n",
        "! python pytorch-fid/src/pytorch_fid/fid_score.py ./pytorch-neural-style-transfer/data/ship1_2 ./pytorch-neural-style-transfer/data/ship3_4_5"
      ],
      "metadata": {
        "id": "xPtWgMiBcHR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6205413-5fe0-406d-c20c-9f1b1fe95b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/pt_inception-2015-12-05-6726825d.pth\n",
            "100% 91.2M/91.2M [00:11<00:00, 8.15MB/s]\n",
            "100% 2/2 [00:21<00:00, 10.84s/it]\n",
            "100% 3/3 [00:08<00:00,  2.99s/it]\n",
            "FID:  203.75461757391366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FID Score comparision between real sonar images of planes and ships \n",
        "! python pytorch-fid/src/pytorch_fid/fid_score.py ./pytorch-neural-style-transfer/data/plane-real ./pytorch-neural-style-transfer/data/plane-real-2"
      ],
      "metadata": {
        "id": "YRy7rx8W7y6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03f4c20f-0a46-4ba9-8c7f-08b88fbaddc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "100% 7/7 [00:25<00:00,  3.66s/it]\n",
            "100% 3/3 [00:05<00:00,  1.87s/it]\n",
            "FID:  170.179870602696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FID Score compirision between two sets of real sonar ship images ? "
      ],
      "metadata": {
        "id": "xMrKB-yrCMtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FID score comparision between ship1_2\n",
        "! python pytorch-fid/src/pytorch_fid/fid_score.py ./pytorch-neural-style-transfer/data/plane-real ./pytorch-neural-style-transfer/data/ship1_2_new"
      ],
      "metadata": {
        "id": "Am4JiShtKgng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a80a8b49-01c8-46c3-807f-76644ee5b61f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "100% 7/7 [00:04<00:00,  1.52it/s]\n",
            "Warning: batch size is bigger than the data size. Setting batch size to data size\n",
            "100% 1/1 [00:16<00:00, 16.61s/it]\n",
            "FID:  363.1913822436733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FID score comparision between ship3_4_5\n",
        "! python pytorch-fid/src/pytorch_fid/fid_score.py ./pytorch-neural-style-transfer/data/plane-real ./pytorch-neural-style-transfer/data/ship3_4_5"
      ],
      "metadata": {
        "id": "Pb47J3j5MC9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8531bbc1-9c6b-42bc-9eda-dc183e26ceaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "100% 7/7 [00:04<00:00,  1.50it/s]\n",
            "100% 3/3 [00:02<00:00,  1.32it/s]\n",
            "FID:  313.8197164999362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DYLIY2WQDk-8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}